---
title: "methyTools - EWAS meta-analysis example"
author:
- name: First Author
  affiliation: First Author's Affiliation
- name: Second Author
  affiliation: Second Author's Affiliation
  email: corresponding@author.com
package: methyTools
output:
   BiocStyle::html_document: 
      toc: true
      toc_float: true
   BiocStyle::pdf_document:
    toc_float: true
  
abstract: |
  Description of your vignette
vignette: |
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup_knitr, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      cache=TRUE, fig.width = 5.5, fig.height = 5.5)
library(knitr)
```

# Prerequisites

The package requires other packages to be installed. These include : `ggplot2`, `VennDiagram`, `RColorBrewer`, `tibble`, `dplyr`, `stringr`, `rasterpdf` and `meta` all disponible in CRAN. The package also requires other packages from Bioconductor to perform annotations : `IlluminaHumanMethylation450kanno.ilmn12.hg19` and `IlluminaHumanMethylationEPICanno.ilm10b4.hg19`

# Overview

In this vignette we will show how to perform an EWAS analysis. As an example we will perform an EWAS analysis with three different cohorts with two distinct models for each cohort


# Getting started

First, let us start by installing and loading required packages

```{r install_dependences, eval=FALSE}
if (!require(rasterpdf, quietly = TRUE)) install.packages('rasterpdf')
if (!require(ggplot2, quietly = TRUE)) install.packages('ggplot2')
if (!require(VennDiagram, quietly = TRUE)) install.packages('VennDiagram')
if (!require(RColorBrewer, quietly = TRUE)) install.packages('RColorBrewer')
if (!require(tibble, quietly = TRUE)) install.packages('tibble')
if (!require(dplyr, quietly = TRUE)) install.packages('dplyr')
if (!require(stringr, quietly = TRUE)) install.packages('stringr')
if (!require(meta, quietly = TRUE)) install.packages('meta') # Forest Plot

if (!requireNamespace("BiocManager", quietly = TRUE))
   install.packages("BiocManager")

BiocManager::install( c("IlluminaHumanMethylation450kanno.ilmn12.hg19", 
                        "IlluminaHumanMethylation450kanno.ilmn12.hg19") )
```

The development version of `methyTools` package can be installed from BRGE GitHub repository:

```{r install_methyTools, eval=FALSE}
devtools::install_github("isglobal-brge/methyTools")
```

```{r load_methyTools, eval=TRUE}
library(methyTools)
library(readtext)
```


# Quality control

## Initial Variables definition


First, we need to define the variables to work with, we will start with files that contains the data to perform analysis. 

### Input data

As we comment before, we will perform an EWAS with three different cohorts with two distinct models for each cohort, so we need to define where the data is stored for each model an each cohort (six files), we do that in a character vector, in that case the variable is called files

```{r QC_varfiles}

files <- c('data/PACE_AQUA_Model1_date_v2.txt',
           'data/PACE_AQUA_Model2_date_v2.txt',
           'data/PACE_INMA_Plate_ModelA1_20170309.txt',
           'data/PACE_INMA_Plate_ModelA2_20170309.txt',
           'data/RICHS_Model1_20170713.txt',
           'data/RICHS_Model2_20170713.txt')
```

files must contain at least the fields : 


probeID   | BETA | SE | P_VAL
------- | ----- | ----- | -----
cg13869341 | 0.00143514362777834 | 0.00963411132344512 | 0.678945643213567
cg24669183 | -0.0215342789035512 | 0.0150948404044624 | 0.01345234123451234
cg15560884 | 0.00156725345562218 | 0.00638784678105962 | 0.845523221223523


### Where store results

We can also define the folder where we will save the results, for example in variable `result_folder`, in this case the results will be stored under QC_Results folder

```{r QC_varres}
# Result folder
results_folder <- 'QC_Results'
```

### Make results understable

to make the analysis more understandable and do not have very complex file names we can define an abbreviated form for each of the files defined above, for example, in future PACE_AQUA_Model1_date_v2 will be treated as PACE_AQUA_A1 or PACE_INMA_Plate_ModelA2_20170309 as PACE_IMMA_A2. The length of the prefix vector must be equal to that of the files

```{r QC_varprefix}
# Prefixes for each file
prefixes <- c('PACE_AQUA_A1', 'PACE_AQUA_A2',
              'PACE_IMMA_A1','PACE_IMMA_A2', 
              'RICHS_A1', 'RICHS_A2')
```


### Illumina Array type and filter conditions

We need to know the Illumina array type , possible values are `450K` and `EPIC`, this data is very important because we filter CpGs attending to Illumina array type

```{r QC_varartype}
# Array type, used : EPIC or 450K
artype <- '450K'
```

In QC process, we exclude that CpGs that not accomplish with defined parameters (Chen et al. (2013)), this parameters are defined in a character vector and are :

  * **MASK_sub25_copy** : indicate whether the 25bp 3'-subsequence of the probe is non-unique
  * **MASK_sub30_copy** : indicate whether the 30bp 3'-subsequence of the probe is non-unique
  * **MASK_sub35_copy** : indicate whether the 35bp 3'-subsequence of the probe is non-unique
  * **MASK_sub40_copy** : indicate whether the 40bp 3'-subsequence of the probe is non-unique
  * **MASK_mapping** : "hether the probe is masked for mapping reason. Probes retained should have high quality (>=40 on 0-60 scale) consistent (with designed MAPINFO) mapping (for both in the case of type I) without INDELs . 
  * **MASK_extBase**  : Probes masked for extension base inconsistent with specified color channel (type-I) or CpG (type-II) based on mapping. 
  * **MASK_typeINextBaseSwitch**  : Whether the probe has a SNP in the extension base that causes a color channel switch from the official annotation (described as color-channel-switching, or CCS SNP in the reference). These probes should be processed differently than designed (by summing up both color channels instead of just the annotated color channel).
  * **MASK_snp5.common**  : Whether 5bp 3'-subsequence (including extension for typeII) overlap with any of the common SNPs from dbSNP (global MAF can be under 1%). 
  * **MASK_snp5.GMAF1p**  : Whether 5bp 3'-subsequence (including extension for typeII) overlap with any of the SNPs with global MAF >1% . 
  * **MASK_general** :  Recommended general purpose masking merged from *"MASK.sub30.copy", "MASK.mapping", "MASK.extBase", "MASK.typeINextBaseSwitch" and "MASK.snp5.GMAF1p"* . 
  * **cpg_probes** :  cpg probes classified as "cg" in the variable named "probeType". 
  * **noncpg_probes** :  non-cpg probes classified as "ch" in the variable named "probeType". 
  * **control_probes** :  control probes classified as "rs" in the variable named "probeType". 
  * **Unreliable_450_EPIC**  : Unreliable probes discordant between 450K and EPIC 
  * **MASK_rmsk15** :  
  * **Sex** :  Keep probes targeting cpgs from sex chromosomes "chrX" and "chrY". ( CpG_chrm %in% "chrX" & CpG_chrm %in% "chrY" )

In this example we exclude CpGs that meet condition : MASK_sub35_copy, MASK_typeINextBaseSwitch, noncpg_probes, control_probes, Unreliable_450_EPIC and Sex.

```{r QC_varexclude}
# Parameters to exclude CpGs
exclude <- c( 'MASK_sub35_copy', 
              'MASK_typeINextBaseSwitch', 
              'noncpg_probes', 
              'control_probes', 
              'Unreliable_450_EPIC', 
              'Sex')
```

We also need to define the ethnic, ethnic can be : EUR SAS AMR GWD YRI TSI  IBS CHS PUR JPT  GIH CH_B STU ITU LWK KHV FIN ESN CEU PJL AC_B CLM CDX GBR BE_B PEL MSL  MXL ASW or  GMAF1p if population is very diverse.

```{r QC_varethnic}
ethnic <- 'EUR'
```


### Other variables :

To obtain the precission plot or perform the GWAMA meta-analysis we need to know the number of samples in data, so we store this information in array N, we define the sample size for each of the files. The n array is similar to N but in that case, we define the size of a dichotomous variable for example smoke

```{r QC_varN}
N <- c(100, 100, 166, 166, 240, 240 )
n <- c(NA)
```

## QC - Numerical analysis code


This code can be executed for each file defined in previous variable `files` but in this example we only show how this analysis work in a simple file.


```{r QC_code_1}

# Variable declaration to perform precision plot
medianSE <- numeric(length(files))
value_N <- numeric(length(files))
cohort_label <- character(length(files))

# Prepare output folder for results (create if not exists)
if(!dir.exists(file.path(getwd(), results_folder )))
   suppressWarnings(dir.create(file.path(getwd(), results_folder)))


# IMPORTANT FOR A REAL ANALYSIS :

# Bucle to analyze all files in the variable files
# for ( i in 1:length(files) )
# {

   # we force i <- 1 to execute the analysis only for the first variable
   # for real data we have to remove this line
   i <- 1

```

First, we need to read the content of a file, 

```{r QC_code_read}
   
# Read data.
cohort <- read.table(files[i], header = TRUE, as.is = TRUE)
print(paste0("Cohort file : ",files[i]," - readed OK", sep = " "))
```

we store the content of the file in a `cohort` variable, after that, we perform a simple descriptive analysis, to do that, we use the function `descriptives_CpGs`, this function needs the data to analyze (`cohort`), the fields that we are interested to get descriptives, in that case BETA, SE and P_VAL (seq(2:4)), and a file name to write results, for the first file will be : *QC_Results/PACE_AQUA_A1_descriptives_init.txt*

```{r QC_code_descriptives}
# Descriptives - Before CpGs deletion
descriptives_CpGs(cohort, seq(2,4), paste0(results_folder,'/',prefixes[i],
                                           '_descriptives_init.txt') )
```

now, we test if there are any duplicate CpGs and if exists, this duplicated CpGs are removed, to do that, we use the function `remove_duplicate_CpGs`, in this function we must indicate what data have to be reviewed and the field that contains the CpGid. Optionally, we can write the duplicates and descriptives related to this duplicates in a file.

```{r QC_code_removedupli}
# Remove duplicates
cohort <- remove_duplicate_CpGs(cohort, "probeID", 
                                paste0(results_folder,'/',prefixes[i],
                                       '_descriptives_duplic.txt'), 
                                paste0(results_folder,'/',prefixes[i],
                                       '_duplicates.txt') )
```

to exclude CpGs that we are not interested in, we use the function `exclude_CpGs`, here we use the parameters defined before in `exclude` variable, the parameters for this function are the data, `cohort`, the CpG id field (can be the column number or the field name) "probeId", the filters to apply, defined in `exclude` variable and optionally, a file name if we want to save excluded CpGs and the exclussion reason, in that the file name will be *QC_Results/PACE_AQUA_A1_excluded.txt*

```{r QC_code_exclCpGs}
# Exclude CpGs not meet conditions
cohort <- exclude_CpGs(cohort, "probeID", exclude, 
                       filename = paste0(results_folder,'/',prefixes[i],
                                         '_excluded.txt') )
```

After eliminating the inconsistent CpGs, we proceed to carry out another descriptive analysis,

```{r QC_code_desclast, eval=FALSE}
# Descriptives - After CpGs deletion #
descriptives_CpGs(cohort, seq(2,4), 
                  paste0(results_folder,'/',prefixes[i],
                         '_descriptives_last.txt') )
```

Now, we can get adjusted p-values by Bonferroni and FDR, the function to adjust data is `adjust_data`, we have to indicate in which column the p-value is and what adjustment we want, by default the function adjust data by Bonferroni (`bn`) and FDR (`fdr`).
This function, returns the input data with two new columns corresponding to this adjustments. As in other functions seen before, optionally, we can get a data summary with the number of significative values with bn, fdr, ....  in a text file, in that case we generate a file *QC_Results/PACE_AQUA_A1_ResumeSignificatives.txt* 

```{r QC_code_adjust}

# data before adjustment
head(cohort)

# Adjust data by Bonferroni and FDR
cohort <- adjust_data(cohort, "P_VAL", bn=TRUE, fdr=TRUE, 
                      filename =  paste0(results_folder,'/',prefixes[i],
                                         '_ResumeSignificatives.txt')  )

# data after adjustment
head(cohort)
```

We are ready to write this data to a file with the `write_QCData` function, the file generated by this function is very important for the next steps, this file is used to get data to generate GWAMA files, this data is stored with *_QC_Data.txt* sufix. In this function data is annotated before being written to the file, 

```{r QC_code_writeQData}
   # Write QC complete data to external file
   write_QCData(cohort, paste0(results_folder,'/',prefixes[i]))
```

## QC - Graphical analysis code 

To perform a graphical analysis we have different functions, we can easily generate a se or p-value distribution plots with `plot_distribution` function

```{r QC_code_distrplot}
   ## Visualization - Plots

   # Distribution plot
   plot_distribution(cohort$SE, main = paste('Standard Errors of', prefixes[i]), xlab = 'SE')
   plot_distribution(cohort$P_VAL, main = paste('p-values of', prefixes[i]), xlab = 'p-value')
```

Or a Volcano plot 

```{r QC_code_qqplot}
   # QQ plot.
   qqman::qq(cohort$P_VAL, main = sprintf('QQ plot of %s (lambda = %f)', prefixes[i], lambda=get_lambda(cohort,"P_VAL")))

   # Volcano plot.
   plot_volcano(cohort, "BETA", "P_VAL", main =paste('Volcano plot of', prefixes[i]) )

```

When we have the results for all models and cohorts, we can perform a Precission plot with `plot_precissionp` function 

```{r invchr17, echo=FALSE, out.width='100%',  fig.align='center',  fig.cap="\\label{fig:invchr17}Precission plot for 10 different datasets ", fig.pos='ht'}
include_graphics("imgs/precision_SE_N.png") 
```

or get the venn diagram with `plot_venndiagram`, to generate a venn diagram we have to define the venn diagram for a maximum of 5 datasets. Here we define which models and cohorts we want to be shown in the Venn diagram. In this example we define two different venn diagrams, one with "PACE_AQUA_A1", "PACE_IMMA_A1" and "RICHS_A1" datasets and the otehr with three more datasets "PACE_AQUA_A2", "PACE_IMMA_A2" and "RICHS_A2"

```{r QC_varvenn, eval=FALSE}
# Venn diagrams
venn_diagrams <- list(
   c("PACE_AQUA_A1", "PACE_IMMA_A1", "RICHS_A1" ),
   c("PACE_AQUA_A2", "PACE_IMMA_A2", "RICHS_A2" )
)
```



# Meta-Analysis with GWAMA

```{r meta_variables}
```

```{r meta_code}
```


# Session info {.unnumbered}

```{r sessionInfo, echo=FALSE}
sessionInfo()
```
