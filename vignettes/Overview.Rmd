---
title: "methyTools - EWAS meta-analysis example"
author:
- name: First Author
  affiliation: First Author's Affiliation
- name: Second Author
  affiliation: Second Author's Affiliation
  email: corresponding@author.com
package: methyTools
output:
   BiocStyle::html_document: 
      toc: true
      toc_float: true
   BiocStyle::pdf_document:
    toc_float: true
  
abstract: |
  Description of your vignette
vignette: |
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup_knitr, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      cache=TRUE, fig.width = 5.5, fig.height = 5.5)
library(knitr)
```

# Prerequisites

The package requires other packages to be installed. These include : `ggplot2`, `VennDiagram`, `RColorBrewer`, `tibble`, `dplyr`, `stringr`, `rasterpdf` and `meta` all disponible in CRAN. The package also requires other packages from Bioconductor to perform annotations : `IlluminaHumanMethylation450kanno.ilmn12.hg19` and `IlluminaHumanMethylationEPICanno.ilm10b4.hg19`.

To perform meta-analyses we use GWAMA, a Software tool for meta analysis developed by Intitute of Genomics from University of Tartu, this software is available at [https://genomics.ut.ee/en/tools/gwama-download] (https://genomics.ut.ee/en/tools/gwama-download), this software must be installed on the computer where we are running analysis

# Overview

In this vignette we will show how to perform an EWAS analysis. As an example we will perform an EWAS analysis with three different cohorts with two distinct models for each cohort


# Getting started

First, let us start by installing and loading required packages

```{r install_dependences, eval=FALSE}
if (!require(rasterpdf, quietly = TRUE)) 
   install.packages('rasterpdf', repos = 'https://cran.rediris.es/' ) 
if (!require(meta, quietly = TRUE)) 
   install.packages('meta', repos = 'https://cran.rediris.es/' ) 
if (!require(ggplot2, quietly = TRUE)) install.packages('ggplot2')
if (!require(VennDiagram, quietly = TRUE)) 
   install.packages('VennDiagram')
if (!require(RColorBrewer, quietly = TRUE)) 
   install.packages('RColorBrewer')
if (!require(tibble, quietly = TRUE)) 
   install.packages('tibble')
if (!require(dplyr, quietly = TRUE)) 
   install.packages('dplyr')
if (!require(stringr, quietly = TRUE)) 
   install.packages('stringr')
if (!require(meta, quietly = TRUE)) 
   install.packages('meta') # Forest Plot
if (!require(missMethyl, quietly = TRUE)) 
   BiocManager::install( "missMethyl" )
if (!require(org.Hs.eg.db, quietly = TRUE)) 
   BiocManager::install( "org.Hs.eg.db" )
if (!require(GenomicRanges, quietly = TRUE)) 
   BiocManager::install( "GenomicRanges" )
if (!require(rtracklayer, quietly = TRUE)) 
   BiocManager::install( "rtracklayer" )

if (!requireNamespace("BiocManager", quietly = TRUE))
   install.packages("BiocManager")

BiocManager::install( c("IlluminaHumanMethylation450kanno.ilmn12.hg19",
                        "IlluminaHumanMethylation450kanno.ilmn12.hg19",
                        "missMethyl",
                        "org.Hs.eg.db",
                        "GenomicRanges") )
```

The development version of `methyTools` package can be installed from BRGE GitHub repository:

```{r install_methyTools, eval=FALSE}
devtools::install_github("isglobal-brge/methyTools")
```

```{r load_methyTools, eval=TRUE}
library(methyTools)
library(readtext)
```


# Quality control

## Initial Variables definition


First, we need to define the variables to work with, we will start with files that contains the data to perform analysis. 

### Input data

As we comment before, we will perform an EWAS with three different cohorts with two distinct models for each cohort, so we need to define where the data is stored for each model an each cohort (six files), we do that in a character vector, in that case the variable is called files

```{r QC_varfiles}

files <- c('data/PACE_AQUA_Model1_date_v2.txt',
           'data/PACE_AQUA_Model2_date_v2.txt',
           'data/PACE_INMA_Plate_ModelA1_20170309.txt',
           'data/PACE_INMA_Plate_ModelA2_20170309.txt',
           'data/RICHS_Model1_20170713.txt',
           'data/RICHS_Model2_20170713.txt')
```

files must contain at least the fields : 


probeID   | BETA | SE | P_VAL
------- | ----- | ----- | -----
cg13869341 | 0.00143514362777834 | 0.00963411132344512 | 0.678945643213567
cg24669183 | -0.0215342789035512 | 0.0150948404044624 | 0.013452341234512
cg15560884 | 0.00156725345562218 | 0.0063878467810596 | 0.845523221223523


### Where store results

We can also define the folder where we will save the results, for example in variable `result_folder`, in this case the results will be stored under QC_Results folder

```{r QC_varres}
# Result folder
results_folder <- 'QC_Results'
```

### Make results understable

to make the analysis more understandable and do not have very complex file names we can define an abbreviated form for each of the files defined above, for example, in future PACE_AQUA_Model1_date_v2 will be treated as PACE_AQUA_A1 or PACE_INMA_Plate_ModelA2_20170309 as PACE_IMMA_A2. The length of the prefix vector must be equal to that of the files

```{r QC_varprefix}
# Prefixes for each file
prefixes <- c('PACE_AQUA_A1', 'PACE_AQUA_A2',
              'PACE_IMMA_A1','PACE_IMMA_A2', 
              'RICHS_A1', 'RICHS_A2')
```


### Illumina Array type and filter conditions

We need to know the Illumina array type , possible values are `450K` and `EPIC`, this data is very important because we filter CpGs attending to Illumina array type

```{r QC_varartype}
# Array type, used : EPIC or 450K
artype <- '450K'
```

In QC process, we exclude that CpGs that not accomplish with defined parameters (Chen et al. (2013)), this parameters are defined in a character vector and are :

  * **MASK_sub25_copy** : indicate whether the 25bp 3'-subsequence of the probe is non-unique
  * **MASK_sub30_copy** : indicate whether the 30bp 3'-subsequence of the probe is non-unique
  * **MASK_sub35_copy** : indicate whether the 35bp 3'-subsequence of the probe is non-unique
  * **MASK_sub40_copy** : indicate whether the 40bp 3'-subsequence of the probe is non-unique
  * **MASK_mapping** : "hether the probe is masked for mapping reason. Probes retained should have high quality (>=40 on 0-60 scale) consistent (with designed MAPINFO) mapping (for both in the case of type I) without INDELs . 
  * **MASK_extBase**  : Probes masked for extension base inconsistent with specified color channel (type-I) or CpG (type-II) based on mapping. 
  * **MASK_typeINextBaseSwitch**  : Whether the probe has a SNP in the extension base that causes a color channel switch from the official annotation (described as color-channel-switching, or CCS SNP in the reference). These probes should be processed differently than designed (by summing up both color channels instead of just the annotated color channel).
  * **MASK_snp5.common**  : Whether 5bp 3'-subsequence (including extension for typeII) overlap with any of the common SNPs from dbSNP (global MAF can be under 1%). 
  * **MASK_snp5.GMAF1p**  : Whether 5bp 3'-subsequence (including extension for typeII) overlap with any of the SNPs with global MAF >1% . 
  * **MASK_general** :  Recommended general purpose masking merged from *"MASK.sub30.copy", "MASK.mapping", "MASK.extBase", "MASK.typeINextBaseSwitch" and "MASK.snp5.GMAF1p"* . 
  * **cpg_probes** :  cpg probes classified as "cg" in the variable named "probeType". 
  * **noncpg_probes** :  non-cpg probes classified as "ch" in the variable named "probeType". 
  * **control_probes** :  control probes classified as "rs" in the variable named "probeType". 
  * **Unreliable_450_EPIC**  : Unreliable probes discordant between 450K and EPIC 
  * **MASK_rmsk15** :  
  * **Sex** :  Keep probes targeting cpgs from sex chromosomes "chrX" and "chrY". ( CpG_chrm %in% "chrX" & CpG_chrm %in% "chrY" )

In this example we exclude CpGs that meet condition : MASK_sub35_copy, MASK_typeINextBaseSwitch, noncpg_probes, control_probes, Unreliable_450_EPIC and Sex.

```{r QC_varexclude}
# Parameters to exclude CpGs
exclude <- c( 'MASK_sub35_copy', 
              'MASK_typeINextBaseSwitch', 
              'noncpg_probes', 
              'control_probes', 
              'Unreliable_450_EPIC', 
              'Sex')
```

We also need to define the ethnic, ethnic can be : EUR SAS AMR GWD YRI TSI  IBS CHS PUR JPT  GIH CH_B STU ITU LWK KHV FIN ESN CEU PJL AC_B CLM CDX GBR BE_B PEL MSL  MXL ASW or  GMAF1p if population is very diverse.

```{r QC_varethnic}
ethnic <- 'EUR'
```


### Other variables :

To obtain the precission plot or perform the GWAMA meta-analysis we need to know the number of samples in data, so we store this information in array N, we define the sample size for each of the files. The n array is similar to N but in that case, we define the size of a dichotomous variable for example smoke

```{r QC_varN}
N <- c(100, 100, 166, 166, 240, 240 )
n <- c(NA)
```

## QC - Numerical analysis code


This code can be executed for each file defined in previous variable `files` but in this example we only show how this analysis work in a simple file.


```{r QC_code_1}

# Variable declaration to perform precision plot
medianSE <- numeric(length(files))
value_N <- numeric(length(files))
cohort_label <- character(length(files))

# Prepare output folder for results (create if not exists)
if(!dir.exists(file.path(getwd(), results_folder )))
   suppressWarnings(dir.create(file.path(getwd(), results_folder)))


# IMPORTANT FOR A REAL ANALYSIS :

# Bucle to analyze all files in the variable files
# for ( i in 1:length(files) )
# {

   # we force i <- 1 to execute the analysis only for the first variable
   # for real data we have to remove this line
   i <- 1

```

First, we need to read the content of a file, 

```{r QC_code_read}
   
# Read data.
cohort <- read.table(files[i], header = TRUE, as.is = TRUE)
print(paste0("Cohort file : ",files[i]," - readed OK", sep = " "))
```

we store the content of the file in a `cohort` variable, after that, we perform a simple descriptive analysis, to do that, we use the function `descriptives_CpGs`, this function needs the data to analyze (`cohort`), the fields that we are interested to get descriptives, in that case BETA, SE and P_VAL (seq(2:4)), and a file name to write results, for the first file will be : *QC_Results/PACE_AQUA_A1_descriptives_init.txt*

```{r QC_code_descriptives}
# Descriptives - Before CpGs deletion
descriptives_CpGs(cohort, seq(2,4), paste0(results_folder,'/',prefixes[i],
                                           '_descriptives_init.txt') )
```

now, we test if there are any duplicate CpGs and if exists, this duplicated CpGs are removed, to do that, we use the function `remove_duplicate_CpGs`, in this function we must indicate what data have to be reviewed and the field that contains the CpGid. Optionally, we can write the duplicates and descriptives related to this duplicates in a file.

```{r QC_code_removedupli}
# Remove duplicates
cohort <- remove_duplicate_CpGs(cohort, "probeID", 
                                paste0(results_folder,'/',prefixes[i],
                                       '_descriptives_duplic.txt'), 
                                paste0(results_folder,'/',prefixes[i],
                                       '_duplicates.txt') )
```

to exclude CpGs that we are not interested in, we use the function `exclude_CpGs`, here we use the parameters defined before in `exclude` variable, the parameters for this function are the data, `cohort`, the CpG id field (can be the column number or the field name) "probeId", the filters to apply, defined in `exclude` variable and optionally, a file name if we want to save excluded CpGs and the exclussion reason, in that the file name will be *QC_Results/PACE_AQUA_A1_excluded.txt*

```{r QC_code_exclCpGs}
# Exclude CpGs not meet conditions
cohort <- exclude_CpGs(cohort, "probeID", exclude, 
                       filename = paste0(results_folder,'/',prefixes[i],
                                         '_excluded.txt') )
```

After eliminating the inconsistent CpGs, we proceed to carry out another descriptive analysis,

```{r QC_code_desclast, eval=FALSE}
# Descriptives - After CpGs deletion #
descriptives_CpGs(cohort, seq(2,4), 
                  paste0(results_folder,'/',prefixes[i],
                         '_descriptives_last.txt') )
```

Now, we can get adjusted p-values by Bonferroni and FDR, the function to adjust data is `adjust_data`, we have to indicate in which column the p-value is and what adjustment we want, by default the function adjust data by Bonferroni (`bn`) and FDR (`fdr`).
This function, returns the input data with two new columns corresponding to this adjustments. As in other functions seen before, optionally, we can get a data summary with the number of significative values with bn, fdr, ....  in a text file, in that case we generate a file *QC_Results/PACE_AQUA_A1_ResumeSignificatives.txt* 

```{r QC_code_adjust}

# data before adjustment
head(cohort)

# Adjust data by Bonferroni and FDR
cohort <- adjust_data(cohort, "P_VAL", bn=TRUE, fdr=TRUE, 
                      filename =  paste0(results_folder,'/',prefixes[i],
                                         '_ResumeSignificatives.txt')  )

# data after adjustment
head(cohort)
```

We are ready to write this data to a file with the `write_QCData` function, the file generated by this function is very important for the next steps, this file is used to get data to generate GWAMA files, this data is stored with *_QC_Data.txt* sufix. In this function data is annotated before being written to the file, 

```{r QC_code_writeQData}
   # Write QC complete data to external file
   write_QCData(cohort, paste0(results_folder,'/',prefixes[i]))
```

## QC - Graphical analysis code 

To perform a graphical analysis we have different functions, we can easily generate a se or p-value distribution plots with `plot_distribution` function

```{r QC_code_distrplot}
   ## Visualization - Plots

   # Distribution plot
   plot_distribution(cohort$SE, 
                     main = paste('Standard Errors of', prefixes[i]), 
                     xlab = 'SE')
   plot_distribution(cohort$P_VAL, 
                     main = paste('p-values of', prefixes[i]), 
                     xlab = 'p-value')
```

Or a Volcano plot 

```{r QC_code_qqplot}
   # QQ plot.
   qqman::qq(cohort$P_VAL,
             main = sprintf('QQ plot of %s (lambda = %f)', prefixes[i], 
                            lambda = get_lambda(cohort,"P_VAL")))

   # Volcano plot.
   plot_volcano(cohort, "BETA", "P_VAL", main=paste('Volcano plot of', prefixes[i]) )

```

When we have the results for all models and cohorts, we can perform a Precission plot with `plot_precissionp` function 

```{r invchr17, echo=FALSE, out.width='100%',  fig.align='center',  fig.cap="\\label{fig:invchr17}Precission plot for 10 different datasets ", fig.pos='ht'}
include_graphics("imgs/precision_SE_N.png") 
```

or get the venn diagram with `plot_venndiagram`, to generate a venn diagram we have to define the venn diagram for a maximum of 5 datasets. Here we define which models and cohorts we want to be shown in the Venn diagram. In this example we define two different venn diagrams, one with "PACE_AQUA_A1", "PACE_IMMA_A1" and "RICHS_A1" datasets and the otehr with three more datasets "PACE_AQUA_A2", "PACE_IMMA_A2" and "RICHS_A2"

```{r QC_varvenn, eval=FALSE}
# Venn diagrams
venn_diagrams <- list(
   c("PACE_AQUA_A1", "PACE_IMMA_A1", "RICHS_A1" ),
   c("PACE_AQUA_A2", "PACE_IMMA_A2", "RICHS_A2" )
)
```



# Meta-Analysis with GWAMA


Like in QC analysis, in Meta-analysis we have to define some variables, one of the variables that we must define is the one that refers to the data of each meta-analysis, for example, in `metafiles` variable we have defined three different meta-analysis, MetaA1, MetaA2 and MetaB, in the first one, MetaA1, we have the datasets 'PACE_AQUA_A1', 'PACE_IMMA_A1' and 'RICHS_A1', we can use the simplified form to make all the study more understandable.

We can also exclude those CpGs with low representation in meta-analysis, we can set the minimum percentage with `pcentMissing` variable, in this example, we take in to account all CpGs not present at least 80% of the datasets of the meta-analysis. We execute the meta-analysis twice,  one with all CpGs and other with only CpGs with precence higer than pcentMissing.

```{r meta_variables, eval=FALSE}
# Define data for each meta-analysis
metafiles <- list(
   'MetaA1' = c('PACE_AQUA_A1','PACE_IMMA_A1', 'RICHS_A1' ),
   'MetaA2' = c('PACE_AQUA_A2','PACE_IMMA_A2', 'RICHS_A2' ),
   'MetaB' = c('PACE_IMMA_B1','PACE_IMMA_B2'))

# Define maximum percent missing for each CpG
pcentMissing <- 0.8 # CpGs with precense lower than pcentMissing after GWAS meta-analysis will be deleted from the study.

```

In this example we only get the first meta-analysis with all CpGs and with CpGs with missing data lower thanpcentMissing but in a complete script all meta-analyzes are performed for both cases, complete and lowCpGs. 

First, we must create the needed folders, in this example we create a GWAMA folder where we will put the input files for GWAMA and GWAMA_Results folder where we will store all the results. 

```{r meta_folders, eval=FALSE}

# Create folder for a meta-analysis in GWAMA folder, here we store 
# the GWAMA input files for each meta-analysis,
# We create one for complete meta-analysis
if(!dir.exists(file.path(getwd(), 
                         paste(folder,"GWAMA", names(metafiles)[metf] ,sep="/") )))
   suppressWarnings(dir.create(file.path(getwd(), 
                                         paste(folder,"GWAMA", 
                                               names(metafiles)[metf], 
                                               sep="/"))))
# We create another for meta-analysis without filtered CpGs 
# with low percentage (sufix _Filtr)
if(!dir.exists(file.path(getwd(), paste0(folder,"/GWAMA/", 
                                         names(metafiles)[metf],"_Filtr") )))
   suppressWarnings(dir.create(file.path(getwd(), 
                                         paste0(folder,"/GWAMA/", 
                                                names(metafiles)[metf],"_Filtr"))))

# GWAMA File name base
inputfolder <- paste0(folder,"/GWAMA/",  names(metafiles)[metf])

modelfiles <- unlist(metafiles[metf])
  
  # runs : Execution with all CpGs and without filtered CpGs (low representation)
   runs <- c('Normal', 'lowcpgs') 
   list.lowCpGs <- NULL
   lowCpGs = FALSE;
   outputfiles <- list()

   outputgwama <- paste(outputfolder,names(metafiles)[metf],sep = '/')
   
```

To perform meta-analyses we use GWAMA, a Software tool for meta analysis developed by Intitute of Genomics from University of Tartu, this software is available at [https://genomics.ut.ee/en/tools/gwama-download] (https://genomics.ut.ee/en/tools/gwama-download), this software must be installed on the computer where we are running analysis and the installation path must be defined in `gwama.dir` variable

```{r meta_vargwama, eval=FALSE}
# GWAMA binary path
#.Original.# gwama.dir <- paste0(Sys.getenv("HOME"), "/data/EWAS_metaanalysis/1_QC_results_cohorts/GWAMA/")
gwama.dir <- "/Users/mailos/tmp/GWAMA_v2/"
```


Now we are ready to execute the analysis, first of all, we need to generate files with predefined format by GWAMA, to do that, we use the function `create_GWAMA_files`, in this function, we have to specify the gwama foder created before, a character vector with models present in meta-analysis (previously defined in metafiles variable), the folder with original data (these are the QC_Data output files from QC), the number of samples in the study and if this is the execution with all CpGs or not, if not, we pass the list with excluded CpGs, we can obtain this list with `get_low_presence_CpGs` function.

`create_GWAMA_files` function takes the original files and converts them to the GWAMA format, it also creates the .ini file necessary to run GWAMA 

When we have all the files ready to execute gwama, we proceed to its execution, with `run_GWAMA_MetaAnalysis` function, this function needs to know : 

* the folder with data to be analyzed, this is the GWAMA folder 
* where to store the results, by default this function creates a subfolder with meta-analysis name and stores all the results together, 
* the meta-analysiss name
* where is the GWAMA binary installed

GWANA is executed by fixed and for random effects, `run_GWAMA_MetaAnalysis` function generates one .out file with meta-analysis results, the Manhattan-plot and the QQplot twice, one for Fixed effects and the other for Random effects. 

```{r meta_analys, eval=FALSE}
   for(j in 1:length(runs))
   {
      if(runs[j]=='lowcpgs') {
         lowCpGs = TRUE
         # Get low presence CpGs in order to exclude this from the new meta-analysis
         list.lowCpGs <- get_low_presence_CpGs(outputfiles[[j-1]], pcentMissing)
         inputfolder <- paste0(folder,"/GWAMA/",  names(metafiles)[metf], "_Filtr")
         outputgwama <- paste0(outputgwama,"_Filtr")
      }

      # Create GWAMA files for each file in meta-analysis and execute GWAMA
      for ( i in 1:length(modelfiles) )
         create_GWAMA_files(folder,  modelfiles[i], inputfolder, N[i], list.lowCpGs )

      #.Original.#outputfiles[[runs[j]]] <- execute_GWAMA_MetaAnalysis(prefixgwama, names(metafiles)[metf])
      outputfiles[[runs[j]]] <- run_GWAMA_MetaAnalysis(inputfolder, 
                                                       outputgwama, 
                                                       names(metafiles)[metf], gwama.dir)

      # Post-metha-analysis QC --- >>> adds BN and FDR adjustment
      ##..## dataPost <- get_descriptives_postGWAMA(outputfolder, outputfiles[[runs[j]]], modelfiles, names(metafiles)[metf], artype )
      dataPost <- get_descriptives_postGWAMA(outputgwama, 
                                             outputfiles[[runs[j]]], 
                                             modelfiles, 
                                             names(metafiles)[metf], artype )

      # Forest-Plot
      plot_ForestPlot( dataPost, metafiles[[metf]], runs[j], inputfolder, names(metafiles)[metf]  )

   }

```

When we have the GWAMA results we perform an analysis with `get_descriptives_postGWAMA` functions, like in QC but with meta-analysis results, we adjust p-values, we annotate CpGs, we generate a file with descriptive result and plot heterogeneity distribution, SE distribution, p-values distribution, QQ-plot with lambda and the volcano plot.

To end with meta-analysis we generate the ForestPlot associated to the first 30 top significative CpGs.


# Enrichment


## Functional enrichment - Based on CpGs

### GO and KEGG


### Functional enrichment - Based on Genes




## Molecular enrichment - Based on CpGs

### Pathways with Molecular Signatures Database (MSigDB)




# Session info {.unnumbered}

```{r sessionInfo, echo=FALSE}
sessionInfo()
```
